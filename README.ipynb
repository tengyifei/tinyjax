{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tinyjax\n",
    "\n",
    "JAX + Tinygrad = Run everywhere\n",
    "\n",
    "This is a prototype tinygrad backend for JAX. Tinygrad is a simple ML framework\n",
    "and compiler that supports many devices such as CUDA, OpenCL, Metal, and even\n",
    "WebGPU and C. JAX is a powerful ML framework that dispatches operations to XLA.\n",
    "By running those operations using Tinygrad instead, we enjoy the extended device\n",
    "support from Tinygrad:\n",
    "\n",
    "- Run JAX on OpenCL (e.g. Intel GPU, AMD GPU without any experimental JAX builds)\n",
    "- Run JAX on Apple Silicon/Metal without experimental prebuilts\n",
    "- Compile JAX to WebGL and WebGPU with fused kernels\n",
    "- Compile JAX to C\n",
    "\n",
    "In fact this notebook is rendered on an Intel laptop with an iGPU. Jax operations\n",
    "are running on the CPU while Tinygrad operations are running on the Intel\n",
    "integrated GPU.\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "The [Tinygrad] API builds a lazy computation graph by tracking what APIs are\n",
    "called on the tensors. When somebody actually needs the data, the graph is JIT\n",
    "compiled into one or more kernels and scheduled on the device (called \"realize\").\n",
    "There are only 25 fundamental operations that everything else lowers into, which\n",
    "makes it very easy to add new backends.\n",
    "\n",
    "JAX can turn a Python function into Jaxpr by tracing it with abstract values,\n",
    "similar to Tinygrad. The resulting Jaxpr is a functional expression language that\n",
    "is strongly related to XLA.\n",
    "\n",
    "We implement a Jaxpr interpreter that translates each Jaxpr operation (such as\n",
    "`dot_general`) into a Tinygrad operation (e.g. `einsum`). Because Tinygrad\n",
    "operations are lazy, the output of the interpreter is a computation graph instead\n",
    "of concrete values. And the graph can be JIT compiled into one big GPU kernel via\n",
    "Tinygrad codegen.\n",
    "\n",
    "## Current state\n",
    "\n",
    "Right now enough ops are implemented to convert a ConvNet (see `ops.py`). But it\n",
    "is very straightforward to add new ops.\n",
    "\n",
    "## Examples\n",
    "\n",
    "The rest of the notebook will show some example conversion.\n",
    "\n",
    "[Tinygrad]: https://github.com/tinygrad/tinygrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GPU', [CpuDevice(id=0)])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"JIT\"] = \"1\"\n",
    "\n",
    "import tinygrad as tg\n",
    "import jax\n",
    "\n",
    "tg.Device.DEFAULT, jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Tensor <LB GPU () float ShapeTracker(views=(View(shape=(), strides=(), offset=0, mask=None, contiguous=True),))> on GPU with grad None>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import tinygrad as tg\n",
    "from tinyjax.decorator import tiny\n",
    "\n",
    "\"\"\"\n",
    "Demonstrate wrapping a JAX function into a Tinygrad function.\n",
    "\"\"\"\n",
    "\n",
    "@tiny\n",
    "def fun(first, second):\n",
    "  temp = first + jnp.sin(second) * 3.0\n",
    "  return jnp.sum(temp)\n",
    "\n",
    "# A lazy computation graph will be returned\n",
    "buf = fun(tg.Tensor([1, 2, 3]), tg.Tensor([4, 5, 6]))\n",
    "buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.01457328, dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can compute (realize) the value using `buf.realize().numpy()`\n",
    "buf.realize().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a simple conv net. I didn't use `relu` since the corresponding\n",
    "higher-order `custom_jvp` JAX primitive isn't implemented in the interpreter yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"Define a CNN and a pure function that runs the CNN.\"\"\"\n",
    "\n",
    "class CNN(nn.Module):\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.silu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.silu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.silu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x\n",
    "\n",
    "bs = 512\n",
    "rng = jax.random.key(42)\n",
    "rng1, rng2 = jax.random.split(rng)\n",
    "cnn = CNN()\n",
    "params = cnn.init(rng1, jnp.ones([1, 28, 28, 1]))\n",
    "images = jax.random.uniform(rng2, (bs, 28, 28, 1))\n",
    "\n",
    "def apply_model(params, images):\n",
    "  \"\"\"This function takes model params and input images and runs the CNN on it.\"\"\"\n",
    "  global cnn\n",
    "  out = cnn.apply(params, images)\n",
    "  assert isinstance(out, jax.Array)\n",
    "  return out\n",
    "\n",
    "apply_model_jax = jax.jit(apply_model)\n",
    "apply_model_tinygrad = tiny(apply_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.test_util\n",
    "\n",
    "# Run the jax version.\n",
    "jax_out = np.array(apply_model_jax(params, images))\n",
    "\n",
    "# Run the tinygrad version.\n",
    "tinygrad_params = jax.tree_util.tree_map(lambda p: tg.Tensor(np.array(p)), params)\n",
    "tinygrad_images = tg.Tensor(np.array(images))\n",
    "tinygrad_out = apply_model_tinygrad(tinygrad_params, tinygrad_images).numpy()\n",
    "\n",
    "assert np.allclose(jax_out, tinygrad_out, rtol=1e-3, atol=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.3 ms ± 622 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit apply_model_tinygrad(tinygrad_params, tinygrad_images).realize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.9 ms ± 1.33 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit apply_model_jax(params, images).block_until_ready()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyjax-t_tWSLfV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
