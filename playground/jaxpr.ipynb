{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototyping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.83602184\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  return jax.numpy.sin(jax.numpy.cos(x))\n",
    "\n",
    "print(f(3.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[]. let b:f32[] = cos a; c:f32[] = sin b in (c,) }\n"
     ]
    }
   ],
   "source": [
    "jaxpr = jax.make_jaxpr(f)(3.0)\n",
    "print(jaxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[8] b:f32[8]. let\n",
      "    c:f32[8] = sin b\n",
      "    d:f32[8] = mul c 3.0\n",
      "    e:f32[8] = add a d\n",
      "    f:f32[] = reduce_sum[axes=(0,)] e\n",
      "  in (f,) }\n"
     ]
    }
   ],
   "source": [
    "from jax import make_jaxpr\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "def func1(first, second):\n",
    "  temp = first + jnp.sin(second) * 3.0\n",
    "  return jnp.sum(temp)\n",
    "\n",
    "\n",
    "print(make_jaxpr(func1)(jnp.zeros(8), jnp.ones(8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda a:f32[6]; b:i32[2] c:f32[2]. let\n",
      "    d:i32[2] = clamp 0 b 2\n",
      "    e:bool[2] = eq d 0\n",
      "    f:f32[2] = stop_gradient c\n",
      "    g:f32[2] = select_n e f c\n",
      "    h:f32[2] = add g 1.0\n",
      "    i:bool[2] = eq d 1\n",
      "    j:f32[2] = stop_gradient c\n",
      "    k:f32[2] = select_n i j c\n",
      "    l:f32[2] = sub k 2.0\n",
      "    m:bool[2] = eq d 2\n",
      "    n:f32[2] = stop_gradient c\n",
      "    o:f32[2] = select_n m n c\n",
      "    p:f32[2] = add o 3.0\n",
      "    q:f32[2] = select_n d h l p\n",
      "    r:f32[] = reduce_sum[axes=(0,)] a\n",
      "    _:f32[2] = add q r\n",
      "    s:f32[2] = broadcast_in_dim[broadcast_dimensions=() shape=(2,)] 1.0\n",
      "    t:bool[2] = eq d 0\n",
      "    u:f32[2] = stop_gradient s\n",
      "    v:f32[2] = select_n t u s\n",
      "    w:bool[2] = eq d 1\n",
      "    x:f32[2] = stop_gradient s\n",
      "    y:f32[2] = select_n w x s\n",
      "    z:bool[2] = eq d 2\n",
      "    ba:f32[2] = stop_gradient s\n",
      "    bb:f32[2] = select_n z ba s\n",
      "    bc:f32[2] = select_n d v y bb\n",
      "  in (bc,) }\n"
     ]
    }
   ],
   "source": [
    "from jax import lax, grad, vmap\n",
    "\n",
    "\n",
    "def one_of_three(index, arg):\n",
    "  a = jnp.array([1, 2, 3, 4, 5, 6], dtype=jnp.float32)\n",
    "  return lax.switch(\n",
    "    index, [lambda x: x + 1.0, lambda x: x - 2.0, lambda x: x + 3.0], arg\n",
    "  ) + jnp.sum(a)\n",
    "\n",
    "\n",
    "func2 = vmap(grad(one_of_three, argnums=1))\n",
    "print(make_jaxpr(func2)(jnp.zeros(2, dtype=jnp.int32) + 1, jnp.zeros(2) + 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from functools import wraps\n",
    "\n",
    "from jax import core\n",
    "from jax import lax\n",
    "from jax._src.util import safe_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ lambda ; a:f32[5]. let b:f32[5] = tanh a; c:f32[5] = exp b in (c,) }\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "  return jnp.exp(jnp.tanh(x))\n",
    "\n",
    "closed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\n",
    "print(closed_jaxpr.jaxpr)\n",
    "print(closed_jaxpr.consts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.core import Jaxpr\n",
    "\n",
    "def eval_jaxpr(jaxpr: Jaxpr, consts, *args, debug: bool = False):\n",
    "  assert type(debug) == bool\n",
    "\n",
    "  # Mapping from variable -> value\n",
    "  env = {}\n",
    "\n",
    "  def read(var):\n",
    "    # Literals are values baked into the Jaxpr\n",
    "    if type(var) is core.Literal:\n",
    "      return var.val\n",
    "    return env[var]\n",
    "\n",
    "  def write(var, val):\n",
    "    env[var] = val\n",
    "    if debug:\n",
    "      print(f\"[JAX] Writing to {var}: {val}\")\n",
    "\n",
    "  # Bind args and consts to environment\n",
    "  safe_map(write, jaxpr.invars, args)\n",
    "  safe_map(write, jaxpr.constvars, consts)\n",
    "\n",
    "  # Loop through equations and evaluate primitives using `bind`\n",
    "  for eqn in jaxpr.eqns:\n",
    "    # Read inputs to equation from environment\n",
    "    invals = safe_map(read, eqn.invars)\n",
    "    if debug:\n",
    "      print(f\"Processing {eqn}\")\n",
    "    # `bind` is how a primitive is called\n",
    "    outvals = eqn.primitive.bind(*invals, **eqn.params)\n",
    "    # Primitives may return multiple outputs or not\n",
    "    if not eqn.primitive.multiple_results:\n",
    "      outvals = [outvals]\n",
    "    # Write the results of the primitive into the environment\n",
    "    safe_map(write, eqn.outvars, outvals)\n",
    "  # Read the final result of the Jaxpr from the environment\n",
    "  return safe_map(read, jaxpr.outvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Array([2.1416876, 2.1416876, 2.1416876, 2.1416876, 2.1416876], dtype=float32)],\n",
       " Array([2.1416876, 2.1416876, 2.1416876, 2.1416876, 2.1416876], dtype=float32))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed_jaxpr = jax.make_jaxpr(f)(jnp.ones(5))\n",
    "eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, jnp.ones(5)), f(jnp.ones(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Array([1., 1.], dtype=float32)], Array([1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "closed_jaxpr = make_jaxpr(func2)(jnp.zeros(2, dtype=jnp.int32) + 1, jnp.zeros(2) + 5)\n",
    "eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, jnp.zeros(2, dtype=jnp.int32) + 1, jnp.zeros(2) + 5), func2(jnp.zeros(2, dtype=jnp.int32) + 1, jnp.zeros(2) + 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example CNN and its jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "  \"\"\"A simple CNN model.\"\"\"\n",
    "\n",
    "  @nn.compact\n",
    "  def __call__(self, x):\n",
    "    x = nn.Conv(features=32, kernel_size=(3, 3))(x)\n",
    "    x = nn.silu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = nn.Conv(features=64, kernel_size=(3, 3))(x)\n",
    "    x = nn.silu(x)\n",
    "    x = nn.avg_pool(x, window_shape=(2, 2), strides=(2, 2))\n",
    "    x = x.reshape((x.shape[0], -1))  # flatten\n",
    "    x = nn.Dense(features=256)(x)\n",
    "    x = nn.silu(x)\n",
    "    x = nn.Dense(features=10)(x)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(cnn: CNN, params, images):\n",
    "  return cnn.apply(params, images)\n",
    "\n",
    "rng = jax.random.key(0)\n",
    "cnn = CNN()\n",
    "params = cnn.init(rng, jnp.ones([1, 28, 28, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.03884823,  0.01811368,  0.05547812, ..., -0.05636393,\n",
       "        -0.00622258,  0.01677726],\n",
       "       [-0.03884823,  0.01811368,  0.05547812, ..., -0.05636393,\n",
       "        -0.00622258,  0.01677726],\n",
       "       [-0.03884823,  0.01811368,  0.05547812, ..., -0.05636393,\n",
       "        -0.00622258,  0.01677726],\n",
       "       ...,\n",
       "       [-0.03884823,  0.01811368,  0.05547812, ..., -0.05636393,\n",
       "        -0.00622258,  0.01677726],\n",
       "       [-0.03884823,  0.01811368,  0.05547812, ..., -0.05636393,\n",
       "        -0.00622258,  0.01677726],\n",
       "       [-0.03884823,  0.01811368,  0.05547812, ..., -0.05636393,\n",
       "        -0.00622258,  0.01677726]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 512\n",
    "images = jnp.ones((bs, 28, 28, 1))\n",
    "\n",
    "def partial_apply(images):\n",
    "  return apply_model(cnn, params, images)\n",
    "\n",
    "partial_apply(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_jaxpr = make_jaxpr(partial_apply)(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ lambda a:f32[3,3,1,32] b:f32[32] c:f32[3,3,32,64] d:f32[64] e:f32[3136,256] f:f32[256]\n",
       "    g:f32[256,10] h:f32[10]; i:f32[512,28,28,1]. let\n",
       "    j:f32[512,28,28,32] = conv_general_dilated[\n",
       "      batch_group_count=1\n",
       "      dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2))\n",
       "      feature_group_count=1\n",
       "      lhs_dilation=(1, 1)\n",
       "      padding=((1, 1), (1, 1))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "      rhs_dilation=(1, 1)\n",
       "      window_strides=(1, 1)\n",
       "    ] i a\n",
       "    k:f32[1,1,1,32] = reshape[dimensions=None new_sizes=(1, 1, 1, 32)] b\n",
       "    l:f32[512,28,28,32] = add j k\n",
       "    m:f32[512,28,28,32] = pjit[\n",
       "      name=silu\n",
       "      jaxpr={ lambda ; n:f32[512,28,28,32]. let\n",
       "          o:f32[512,28,28,32] = logistic n\n",
       "          p:f32[512,28,28,32] = mul n o\n",
       "        in (p,) }\n",
       "    ] l\n",
       "    q:f32[512,14,14,32] = reduce_window_sum[\n",
       "      base_dilation=(1, 1, 1, 1)\n",
       "      padding=((0, 0), (0, 0), (0, 0), (0, 0))\n",
       "      window_dilation=(1, 1, 1, 1)\n",
       "      window_dimensions=(1, 2, 2, 1)\n",
       "      window_strides=(1, 2, 2, 1)\n",
       "    ] m\n",
       "    r:f32[] = convert_element_type[new_dtype=float32 weak_type=False] 4\n",
       "    s:f32[512,14,14,32] = div q r\n",
       "    t:f32[512,14,14,64] = conv_general_dilated[\n",
       "      batch_group_count=1\n",
       "      dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2))\n",
       "      feature_group_count=1\n",
       "      lhs_dilation=(1, 1)\n",
       "      padding=((1, 1), (1, 1))\n",
       "      precision=None\n",
       "      preferred_element_type=None\n",
       "      rhs_dilation=(1, 1)\n",
       "      window_strides=(1, 1)\n",
       "    ] s c\n",
       "    u:f32[1,1,1,64] = reshape[dimensions=None new_sizes=(1, 1, 1, 64)] d\n",
       "    v:f32[512,14,14,64] = add t u\n",
       "    w:f32[512,14,14,64] = pjit[\n",
       "      name=silu\n",
       "      jaxpr={ lambda ; x:f32[512,14,14,64]. let\n",
       "          y:f32[512,14,14,64] = logistic x\n",
       "          z:f32[512,14,14,64] = mul x y\n",
       "        in (z,) }\n",
       "    ] v\n",
       "    ba:f32[512,7,7,64] = reduce_window_sum[\n",
       "      base_dilation=(1, 1, 1, 1)\n",
       "      padding=((0, 0), (0, 0), (0, 0), (0, 0))\n",
       "      window_dilation=(1, 1, 1, 1)\n",
       "      window_dimensions=(1, 2, 2, 1)\n",
       "      window_strides=(1, 2, 2, 1)\n",
       "    ] w\n",
       "    bb:f32[] = convert_element_type[new_dtype=float32 weak_type=False] 4\n",
       "    bc:f32[512,7,7,64] = div ba bb\n",
       "    bd:f32[512,3136] = reshape[dimensions=None new_sizes=(512, 3136)] bc\n",
       "    be:f32[512,256] = dot_general[dimension_numbers=(([1], [0]), ([], []))] bd e\n",
       "    bf:f32[1,256] = reshape[dimensions=None new_sizes=(1, 256)] f\n",
       "    bg:f32[512,256] = add be bf\n",
       "    bh:f32[512,256] = pjit[\n",
       "      name=silu\n",
       "      jaxpr={ lambda ; bi:f32[512,256]. let\n",
       "          bj:f32[512,256] = logistic bi\n",
       "          bk:f32[512,256] = mul bi bj\n",
       "        in (bk,) }\n",
       "    ] bg\n",
       "    bl:f32[512,10] = dot_general[dimension_numbers=(([1], [0]), ([], []))] bh g\n",
       "    bm:f32[1,10] = reshape[dimensions=None new_sizes=(1, 10)] h\n",
       "    bn:f32[512,10] = add bl bm\n",
       "  in (bn,) }"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from jax.interpreters.partial_eval import dce_jaxpr\n",
    "\n",
    "opt_jaxpr, _ = dce_jaxpr(closed_jaxpr.jaxpr, [True])\n",
    "opt_jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert jnp.all(eval_jaxpr(opt_jaxpr, closed_jaxpr.literals, images)[0] == partial_apply(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all operations in this jaxpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "add\n",
      "div\n",
      "conv_general_dilated\n",
      "mul\n",
      "reshape\n",
      "dot_general\n",
      "logistic\n",
      "convert_element_type\n",
      "reduce_window_sum\n"
     ]
    }
   ],
   "source": [
    "from jax._src.core import AxisPrimitive\n",
    "\n",
    "def find_ops(jaxpr):\n",
    "  prims = set()\n",
    "\n",
    "  def add_prims(prims, jaxpr):\n",
    "    for eqn in jaxpr.eqns:\n",
    "      match eqn.primitive:\n",
    "        case AxisPrimitive(name=\"pjit\") | AxisPrimitive(name=\"jit\"):\n",
    "          # Recursively process sub-jaxpr\n",
    "          add_prims(prims, eqn.params[\"jaxpr\"])\n",
    "        case _:\n",
    "          prims.add(eqn.primitive)\n",
    "\n",
    "  add_prims(prims, jaxpr)\n",
    "  return prims\n",
    "\n",
    "prims = find_ops(opt_jaxpr)\n",
    "for p in prims:\n",
    "  print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy numpy interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{add, convert_element_type, mul, reduce_sum, sin}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func1(first, second):\n",
    "  temp = first + jnp.sin(second) * 3.0\n",
    "  return jnp.sum(temp)\n",
    "\n",
    "closed_jaxpr = make_jaxpr(func1)(5, jnp.array([1, 2, 3]))\n",
    "find_ops(closed_jaxpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Sequence\n",
    "from jax.core import Jaxpr\n",
    "from jax.typing import DTypeLike\n",
    "from jax import lax\n",
    "\n",
    "np_registry = {}\n",
    "\n",
    "def np_convert_element_type(operand: Any, new_dtype: DTypeLike, weak_type: bool = False) -> np.ndarray:\n",
    "  match operand:\n",
    "    case int() | float() | np.floating():\n",
    "      return np.array(operand).astype(new_dtype)\n",
    "    case _:\n",
    "      return operand.astype(new_dtype)\n",
    "np_registry[lax.convert_element_type_p] = np_convert_element_type\n",
    "\n",
    "def np_sin(x: np.ndarray) -> np.ndarray:\n",
    "  return np.sin(x)\n",
    "np_registry[lax.sin_p] = np_sin\n",
    "\n",
    "def np_mul(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "  return np.multiply(a, b)\n",
    "np_registry[lax.mul_p] = np_mul\n",
    "\n",
    "def np_add(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "  return np.add(a, b)\n",
    "np_registry[lax.add_p] = np_add\n",
    "\n",
    "def np_reduce_sum(operand: np.ndarray, axes: Sequence[int]) -> np.ndarray:\n",
    "  return np.sum(operand, axes)\n",
    "np_registry[lax.reduce_sum_p] = np_reduce_sum\n",
    "\n",
    "def eval_jaxpr_np(jaxpr: Jaxpr, consts, *args):\n",
    "  # Mapping from variable -> value\n",
    "  env = {}\n",
    "\n",
    "  def read(var):\n",
    "    # Literals are values baked into the Jaxpr\n",
    "    if type(var) is core.Literal:\n",
    "      return var.val\n",
    "    return env[var]\n",
    "\n",
    "  def write(var, val):\n",
    "    match val:\n",
    "      case int() | float() | np.floating():\n",
    "        env[var] = np.array(val)\n",
    "      case jax.Array():\n",
    "        env[var] = np.array(val)\n",
    "      case _:\n",
    "        env[var] = val\n",
    "\n",
    "  # Bind args and consts to environment\n",
    "  safe_map(write, jaxpr.invars, args)\n",
    "  safe_map(write, jaxpr.constvars, consts)\n",
    "\n",
    "  # Loop through equations and evaluate primitives\n",
    "  for eqn in jaxpr.eqns:\n",
    "    # Read inputs to equation from environment\n",
    "    invals = safe_map(read, eqn.invars)\n",
    "    if eqn.primitive not in np_registry:\n",
    "      raise NotImplementedError(\n",
    "          f\"{eqn.primitive} does not have an implementation\")\n",
    "    outvals = np_registry[eqn.primitive](*invals, **eqn.params)\n",
    "    # Primitives may return multiple outputs or not\n",
    "    if not eqn.primitive.multiple_results:\n",
    "      outvals = [outvals]\n",
    "    # Write the results of the primitive into the environment\n",
    "    safe_map(write, eqn.outvars, outvals)\n",
    "  # Read the final result of the Jaxpr from the environment\n",
    "  return safe_map(read, jaxpr.outvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(20.675665, dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Array(20.675665, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "print(eval_jaxpr_np(closed_jaxpr.jaxpr, closed_jaxpr.consts, 5, jnp.array([1, 2, 3])))\n",
    "print(eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, 5, jnp.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinygrad interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tinygrad as tg\n",
    "import tinygrad.dtype\n",
    "import tinygrad.function as tgf\n",
    "from typing import Any, Sequence, Tuple\n",
    "from jax.core import Jaxpr, ShapedArray, ClosedJaxpr\n",
    "from jax.typing import DTypeLike\n",
    "from jax import lax\n",
    "from jax._src.typing import Shape\n",
    "from jax._src.pjit import pjit_p\n",
    "\n",
    "tg_registry = {}\n",
    "\n",
    "# mul\n",
    "def tg_mul(a: tg.Tensor, b: tg.Tensor) -> tg.Tensor:\n",
    "  return a.mul(b)\n",
    "tg_registry[lax.mul_p] = tg_mul\n",
    "\n",
    "# reshape\n",
    "def tg_reshape(operand: tg.Tensor, new_sizes: Shape, dimensions: Sequence[int] | None = None) -> tg.Tensor:\n",
    "  shape = list(int(v) for v in new_sizes)\n",
    "  if dimensions:\n",
    "    permuted_shape = [0] * len(dimensions)\n",
    "    for i, d in enumerate(dimensions):\n",
    "      permuted_shape[d] = shape[i]\n",
    "    shape = permuted_shape\n",
    "  return operand.reshape(shape)\n",
    "tg_registry[lax.reshape_p] = tg_reshape\n",
    "\n",
    "# div\n",
    "def tg_div(a: tg.Tensor, b: tg.Tensor) -> tg.Tensor:\n",
    "  return a.div(b)\n",
    "tg_registry[lax.div_p] = tg_div\n",
    "\n",
    "# reduce_window_sum\n",
    "def tg_reduce_window_sum(operand: tg.Tensor, window_dimensions: Shape,\n",
    "                         window_strides: Sequence[int],\n",
    "                         padding: Sequence[tuple[int, int]],\n",
    "                         base_dilation: Sequence[int] | None = None,\n",
    "                         window_dilation: Sequence[int] | None = None) -> tg.Tensor:\n",
    "  # a:f32[3,14,14,32] = reduce_window_sum[\n",
    "  #   base_dilation=(1, 1, 1, 1)\n",
    "  #   padding=((0, 0), (0, 0), (0, 0), (0, 0))\n",
    "  #   window_dilation=(1, 1, 1, 1)\n",
    "  #   window_dimensions=(1, 2, 2, 1)\n",
    "  #   window_strides=(1, 2, 2, 1)\n",
    "  # ]\n",
    "  assert padding == ((0, 0), (0, 0), (0, 0), (0, 0))\n",
    "  assert base_dilation == (1, 1, 1, 1)\n",
    "  window_dilation = window_dilation or [1, 1, 1, 1]\n",
    "  pooled = operand._pool(\n",
    "    k_=tuple(window_dimensions), stride=tuple(window_strides), dilation=tuple(window_dilation))\n",
    "  return pooled.sum(axis=tuple(range(0-len(window_dimensions), 0)))\n",
    "tg_registry[lax.reduce_window_sum_p] = tg_reduce_window_sum\n",
    "\n",
    "# reduce_sum\n",
    "def tg_reduce_sum(operand: tg.Tensor, axes: Sequence[int]) -> tg.Tensor:\n",
    "  return operand._reduce(tgf.Sum, tuple(axes))\n",
    "tg_registry[lax.reduce_sum_p] = tg_reduce_sum\n",
    "\n",
    "# convert_element_type\n",
    "def tg_convert_element_type(operand: Any, new_dtype: DTypeLike, weak_type: bool = False) -> tg.Tensor:\n",
    "  match operand:\n",
    "    case int() | float() | np.floating() | np.integer():\n",
    "      return tg.Tensor(np.array(operand).astype(new_dtype))\n",
    "    case tg.Tensor():\n",
    "      return operand.cast(convert_dtype(np.dtype(new_dtype)))\n",
    "    case _:\n",
    "      raise RuntimeError(f\"Unsupported operand type {type(operand)}\")\n",
    "def convert_dtype(dtype: np.dtype) -> tinygrad.dtype.DType:\n",
    "  match dtype:\n",
    "    case np.float16:\n",
    "      return tinygrad.dtypes.float16\n",
    "    case np.float32:\n",
    "      return tinygrad.dtypes.float32\n",
    "    case np.int32:\n",
    "      return tinygrad.dtypes.int32\n",
    "    case np.int64:\n",
    "      return tinygrad.dtypes.int64\n",
    "    case _:\n",
    "      raise RuntimeError(f\"Unsupported dtype f{dtype}\")\n",
    "tg_registry[lax.convert_element_type_p] = tg_convert_element_type\n",
    "\n",
    "# add\n",
    "def tg_add(a: tg.Tensor, b: tg.Tensor) -> tg.Tensor:\n",
    "  return a.add(b)\n",
    "tg_registry[lax.add_p] = tg_add\n",
    "\n",
    "# logistic\n",
    "def tg_logistic(x: tg.Tensor) -> tg.Tensor:\n",
    "  return x.sigmoid()\n",
    "tg_registry[lax.logistic_p] = tg_logistic\n",
    "\n",
    "# sin\n",
    "def tg_sin(x: tg.Tensor) -> tg.Tensor:\n",
    "  return x.sin()\n",
    "tg_registry[lax.sin_p] = tg_sin\n",
    "\n",
    "# dot_general\n",
    "def tg_dot_general(lhs: tg.Tensor, rhs: tg.Tensor, dimension_numbers: lax.DotDimensionNumbers,\n",
    "                   precision: lax.PrecisionLike = None,\n",
    "                   preferred_element_type: DTypeLike | None = None) -> tg.Tensor:\n",
    "  # a:f32[3,256] = dot_general[dimension_numbers=(([1], [0]), ([], []))] b c\n",
    "  assert precision is None\n",
    "  assert preferred_element_type is None\n",
    "\n",
    "  ((lhs_contracting_dims, rhs_contracting_dims), (lhs_batch_dims, rhs_batch_dims)) = dimension_numbers\n",
    "  # Generate a set of unique labels for einsum\n",
    "  labels = 'abcdefghijklmnopqrstuvwxyz'\n",
    "\n",
    "  # Assign labels to each dimension of lhs and rhs\n",
    "  lhs_labels = [''] * len(lhs.shape)\n",
    "  rhs_labels = [''] * len(rhs.shape)\n",
    "\n",
    "  # Assign labels for contracting dimensions\n",
    "  for i, (lhs_dim, rhs_dim) in enumerate(zip(lhs_contracting_dims, rhs_contracting_dims)):\n",
    "    label = labels[i]\n",
    "    lhs_labels[lhs_dim] = label\n",
    "    rhs_labels[rhs_dim] = label\n",
    "\n",
    "  # Start index for batch and other dimensions\n",
    "  start_idx = len(lhs_contracting_dims)\n",
    "\n",
    "  # Assign labels for batch dimensions\n",
    "  for i, (lhs_dim, rhs_dim) in enumerate(zip(lhs_batch_dims, rhs_batch_dims), start=start_idx):\n",
    "    label = labels[i]\n",
    "    lhs_labels[lhs_dim] = label\n",
    "    rhs_labels[rhs_dim] = label\n",
    "\n",
    "  # Assign labels for remaining dimensions\n",
    "  for i, label in enumerate(lhs_labels):\n",
    "    if label == '':\n",
    "      lhs_labels[i] = labels[start_idx]\n",
    "      start_idx += 1\n",
    "\n",
    "  for i, label in enumerate(rhs_labels):\n",
    "    if label == '':\n",
    "      rhs_labels[i] = labels[start_idx]\n",
    "      start_idx += 1\n",
    "\n",
    "  # Construct the einsum string\n",
    "  lhs_subscripts = ''.join(lhs_labels)\n",
    "  rhs_subscripts = ''.join(rhs_labels)\n",
    "  result_subscripts = ''.join([label for label in lhs_labels + rhs_labels if label not in lhs_labels or label not in rhs_labels])\n",
    "\n",
    "  formula = f'{lhs_subscripts},{rhs_subscripts}->{result_subscripts}'\n",
    "  return tg.Tensor.einsum(formula, lhs, rhs)\n",
    "tg_registry[lax.dot_general_p] = tg_dot_general\n",
    "\n",
    "# conv_general_dilated\n",
    "def tg_conv_general_dilated(\n",
    "  lhs: tg.Tensor, rhs: tg.Tensor, window_strides: Sequence[int],\n",
    "  padding: str | Sequence[tuple[int, int]],\n",
    "  lhs_dilation: Sequence[int] | None = None,\n",
    "  rhs_dilation: Sequence[int] | None = None,\n",
    "  dimension_numbers: lax.ConvGeneralDilatedDimensionNumbers  = None,\n",
    "  feature_group_count: int = 1, batch_group_count: int = 1,\n",
    "  precision: lax.PrecisionLike = None,\n",
    "  preferred_element_type: DTypeLike | None = None\n",
    ") -> tg.Tensor:\n",
    "  # t:f32[3,14,14,64] = conv_general_dilated[\n",
    "  #    batch_group_count=1\n",
    "  #    dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2))\n",
    "  #    feature_group_count=1\n",
    "  #    lhs_dilation=(1, 1)\n",
    "  #    padding=((1, 1), (1, 1))\n",
    "  #    precision=None\n",
    "  #    preferred_element_type=None\n",
    "  #    rhs_dilation=(1, 1)\n",
    "  #    window_strides=(1, 1)\n",
    "  #  ]\n",
    "  dim_nums = lax.conv_dimension_numbers(lhs.shape, rhs.shape, dimension_numbers)\n",
    "  lhs = lhs.permute(dim_nums.lhs_spec)\n",
    "  rhs = rhs.permute(dim_nums.rhs_spec)\n",
    "  assert batch_group_count == 1\n",
    "  assert feature_group_count == 1\n",
    "  assert precision is None\n",
    "  assert preferred_element_type is None\n",
    "  assert lhs_dilation == (1, 1)\n",
    "  assert not isinstance(padding, str)\n",
    "  assert len(padding) == 2\n",
    "  assert padding[0][0] == padding[0][1]\n",
    "  assert padding[1][0] == padding[1][1]\n",
    "  tg_padding = (int(padding[0][0]), int(padding[1][0]))\n",
    "  assert window_strides == (1, 1)\n",
    "  result = lhs.conv2d(rhs, dilation=tuple(int(x) for x in rhs_dilation), padding=tg_padding) # type: ignore\n",
    "  return permute_to_spec(result, existing=(0, 2, 3, 1), desired=dim_nums.out_spec)\n",
    "def permute_to_spec(v: tg.Tensor, existing: Sequence[int], desired: Sequence[int]) -> tg.Tensor:\n",
    "  assert all(0 <= i <= 3 for i in existing)\n",
    "  assert all(0 <= i <= 3 for i in desired)\n",
    "  permute_idx = tuple(int(existing.index(i)) for i in desired)\n",
    "  return v.permute(permute_idx)\n",
    "tg_registry[lax.conv_general_dilated_p] = tg_conv_general_dilated\n",
    "\n",
    "# pjit\n",
    "def tg_pjit(*inputs, jaxpr: ClosedJaxpr, **kwargs):\n",
    "  return eval_jaxpr_tg(jaxpr.jaxpr, jaxpr.consts, *inputs)\n",
    "tg_registry[pjit_p] = tg_pjit\n",
    "\n",
    "def eval_jaxpr_tg(jaxpr: Jaxpr, consts, *args, debug: bool = False):\n",
    "  assert type(debug) == bool\n",
    "\n",
    "  # Mapping from variable -> value\n",
    "  env = {}\n",
    "\n",
    "  def read(var):\n",
    "    # Literals are values baked into the Jaxpr\n",
    "    if type(var) is core.Literal:\n",
    "      return tg.Tensor(np.array(var.val))\n",
    "    return env[var]\n",
    "\n",
    "  def write(var, val):\n",
    "    match val:\n",
    "      case int() | float() | np.floating() | np.integer():\n",
    "        env[var] = tg.Tensor(np.array(val))\n",
    "      case jax.Array():\n",
    "        env[var] = tg.Tensor(np.array(val))\n",
    "      case _:\n",
    "        env[var] = val\n",
    "    if debug:\n",
    "      print(f\"Writing to {var}: {env[var].numpy()}\")\n",
    "\n",
    "  # Bind args and consts to environment\n",
    "  safe_map(write, jaxpr.invars, args)\n",
    "  safe_map(write, jaxpr.constvars, consts)\n",
    "\n",
    "  # Loop through equations and evaluate primitives\n",
    "  for eqn in jaxpr.eqns:\n",
    "    # Read inputs to equation from environment\n",
    "    invals = safe_map(read, eqn.invars)\n",
    "    if eqn.primitive not in tg_registry:\n",
    "      raise NotImplementedError(\n",
    "          f\"{eqn.primitive} does not have an implementation: {eqn}\")\n",
    "    if debug:\n",
    "      print(f\"Processing {eqn}\")\n",
    "    outvals = tg_registry[eqn.primitive](*invals, **eqn.params)\n",
    "    # Primitives may return multiple outputs or not\n",
    "    if not eqn.primitive.multiple_results:\n",
    "      outvals = [outvals]\n",
    "    for var, val in zip(eqn.outvars, outvals):\n",
    "      if isinstance(var.aval, ShapedArray):\n",
    "        assert var.aval.shape == val.shape, f\"{var.aval.shape} != {val.shape}\"\n",
    "    # Write the results of the primitive into the environment\n",
    "    safe_map(write, eqn.outvars, outvals)\n",
    "\n",
    "  # Read the final result of the Jaxpr from the environment\n",
    "  return safe_map(read, jaxpr.outvars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(20.675665, dtype=float32)]\n",
      "[Array(20.675665, dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "def func1(first, second):\n",
    "  temp = first + jnp.sin(second) * 3.0\n",
    "  return jnp.sum(temp)\n",
    "\n",
    "closed_jaxpr = make_jaxpr(func1)(5, jnp.array([1, 2, 3]))\n",
    "find_ops(closed_jaxpr)\n",
    "\n",
    "print(list(map(lambda x: x.numpy(), eval_jaxpr_tg(closed_jaxpr.jaxpr, closed_jaxpr.consts, 5, jnp.array([1, 2, 3])))))\n",
    "print(eval_jaxpr(closed_jaxpr.jaxpr, closed_jaxpr.consts, 5, jnp.array([1, 2, 3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "closed_jaxpr = make_jaxpr(partial_apply)(images)\n",
    "opt_jaxpr, _ = dce_jaxpr(closed_jaxpr.jaxpr, [True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tinygrad:  [[-0.03884818  0.01811363  0.05547819 ... -0.05636396 -0.00622255\n",
      "   0.01677731]\n",
      " [-0.03884818  0.01811363  0.05547819 ... -0.05636396 -0.00622255\n",
      "   0.01677731]\n",
      " [-0.03884818  0.01811363  0.05547819 ... -0.05636396 -0.00622255\n",
      "   0.01677731]\n",
      " ...\n",
      " [-0.03884818  0.01811363  0.05547819 ... -0.05636396 -0.00622255\n",
      "   0.01677731]\n",
      " [-0.03884818  0.01811363  0.05547819 ... -0.05636396 -0.00622255\n",
      "   0.01677731]\n",
      " [-0.03884818  0.01811363  0.05547819 ... -0.05636396 -0.00622255\n",
      "   0.01677731]]\n",
      "jax:  [[-0.03884823  0.01811368  0.05547812 ... -0.05636393 -0.00622258\n",
      "   0.01677726]\n",
      " [-0.03884823  0.01811368  0.05547812 ... -0.05636393 -0.00622258\n",
      "   0.01677726]\n",
      " [-0.03884823  0.01811368  0.05547812 ... -0.05636393 -0.00622258\n",
      "   0.01677726]\n",
      " ...\n",
      " [-0.03884823  0.01811368  0.05547812 ... -0.05636393 -0.00622258\n",
      "   0.01677726]\n",
      " [-0.03884823  0.01811368  0.05547812 ... -0.05636393 -0.00622258\n",
      "   0.01677726]\n",
      " [-0.03884823  0.01811368  0.05547812 ... -0.05636393 -0.00622258\n",
      "   0.01677726]]\n"
     ]
    }
   ],
   "source": [
    "tg_out = eval_jaxpr_tg(opt_jaxpr, closed_jaxpr.literals, images)[0].numpy()\n",
    "jax_out = np.array(eval_jaxpr(opt_jaxpr, closed_jaxpr.literals, images)[0])\n",
    "print(\"tinygrad: \", tg_out)\n",
    "print(\"jax: \", jax_out)\n",
    "assert np.allclose(tg_out, jax_out, rtol=1e-2, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygrad import TinyJit\n",
    "\n",
    "@TinyJit\n",
    "def eval_tg(images):\n",
    "  return eval_jaxpr_tg(opt_jaxpr, closed_jaxpr.literals, images)[0].realize()\n",
    "\n",
    "@jax.jit\n",
    "def eval_jax(images):\n",
    "  return eval_jaxpr(opt_jaxpr, closed_jaxpr.literals, images)[0]\n",
    "\n",
    "random_images = [ np.random.rand(bs, 28, 28, 1).astype(np.float32) for _ in range(100) ]\n",
    "random_images_tg = [ tg.Tensor(x) for x in random_images ]\n",
    "random_images_jax = [ jax.numpy.array(x) for x in random_images ]\n",
    "\n",
    "def run_tg():\n",
    "  return [eval_tg(x).numpy() for x in random_images_tg]\n",
    "\n",
    "def run_jax():\n",
    "  return [np.array(eval_jax(x)) for x in random_images_jax]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tinygrad on OpenCL/GPU vs JAX on CPU\n",
    "\n",
    "CPU: 11th Gen Intel i7-11850H (16) @ 4.800GHz\n",
    "\n",
    "GPU: Intel TigerLake-H GT1 [UHD Graphics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.51 s ± 121 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_tg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.07 s ± 56.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run_jax()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tinyjax-t_tWSLfV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
